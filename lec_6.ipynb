{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The QMSS program gives students access to some of the most accomplished scholars at Columbia University Working with a broad range of faculty members on theses and other research projects provides our students with the intellectual and material resources they need to accomplish their educational goals Listed below are the faculty currently teaching in the QMSS program but students also work with a wide range of faculty across the university    Dr Gelman is the founder of the QMSS program He still maintains close ties with QMSS faculty projects and students He is currently a professor of statistics and political science and director of the Applied Statistics Center at Columbia University He has received the Outstanding Statistical Application award from the American Statistical Association the award for best article published in the American Political Science Review and the Council of Presidents of Statistical Societies award for outstanding contributions by a person under the age of His books include Bayesian Data Analysis with John Carlin Hal Stern David Dunson Aki Vehtari and Don Rubin Teaching Statistics A Bag of Tricks with Deb Nolan Data Analysis Using Regression and Multilevel Hierarchical Models with Jennifer Hill Red State Blue State Rich State Poor State Why Americans Vote the Way They Do with David Park Boris Shor and Jeronimo Cortina and A Quantitative Tour of the Social Sciences co edited with Jeronimo Cortina  Dr Connelly works in international and global history He received his B A from Columbia and his Ph D from Yale His publications include A Diplomatic Revolution Algeria s Fight for Independence and the Origins of the Post Cold War Era and Fatal Misconception The Struggle to Control World Population He has written research articles in Comparative Studies in Society and History The International Journal of Middle East Studies The American Historical Review The Review francaise d histoire d Outre mer and Past Present He has also published commentary on international affairs in The Atlantic Monthly and The National Interest  Dr Eirich is the Director of the Quantitative Methods in the Social Sciences QMSS MA Program and is appointed Lecturer in Discipline within the Department of Sociology His course offerings include Data Analysis Advanced Analytic Techniques Research Seminar Time Series and Social Network Analysis with QMSS He researches the causes and consequences of socioeconomic inequality with a particular focus on family processes He has studied rich get richer dynamics in the CEO labor market and the cumulative academic consequences of reading ability groups in the early education His dissertation examined the relationship between parental religiosity and children s educational attainment in the United States He has many on going projects in collaboration with MA and Ph D students His work has appeared in the American Journal of Sociology with Thomas Diprete and Matthew Pittinsky Annual Review of Sociology with Thomas DiPrete International Journal of the Sociology of the Family Research in the Sociology of Work in Adolescence in the st Century Constants and Challenges eds Frances R Spielhagen Paul D Schwartz Information Age Publishing and most recently in the Journal of Family Issues He has a BA in Classical Languages and Philosophy from Fordham University and his Ph D is from Columbia in Sociology Prior to teaching Greg was a senior consultant conducting health care research at The Advisory Board Company in Washington DC He can be reached via email  Elena Krumova is the Assistant Director of the QMSS Program She holds a PhD in Sociology from Columbia University and has several years of experience teaching at the undergraduate and graduate levels Previously she has been a Visiting Assistant Professor at the School of Public Policy at Central European University Budapest and a Post doctoral fellow at the Harriman Institute at Columbia University Elena has a lot of experience advising students and over the last decade she has helped scores of students conceive of and complete complex research projects  Meghan is the Program Coordinator for QMSS She studied Theatre and Environmental Policy at Northwestern University where she subsequently worked as a Research Coordinator for the Science of Networks in Communities Research Group She has been with the QMSS program since and is responsible for coordinating all academic operations and enrichment activities Beyond QMSS she is involved with a number education outreach organizations within New York City She is the primary contact for the program and can be reached by phone at or email at mdm columbia edu  Dr Goodrich is a core instructor of QMSS and teaches Missing Data Bayesian Statistics Data Mining Data Analysis and Theory and Methodology at QMSS Previously he was a Post doctoral Researcher working with Andrew Gelman at the Applied Statistics Center at Columbia University primarily on the mi R package for missing Data He received his PhD in Government and Social Policy from Harvard University in where his dissertation It s Not All About the Benjamins Political Economy and Social Psychology Theories of Welfare State Preferences derived two new estimators and applies them to cross country survey data to test competing theories of preferences for redistribution and other welfare state programs He previously served as a research assistant at the Peterson Institute for international Economics His research interests include methodology comparative politics and political economy  Dr Parrott is appointed Lecturer in Discipline within the Department of Political Science and he teaches GIS and Spatial Analysis Theory and Methods Data Analysis for the Social Sciences and Data Visualization with QMSS Prior to joining the QMSS faculty he was a American Political Science Association Congressional Fellow As an APSA fellow he designed web applications to organize centralize and automate data collection and everyday tasks for committee and personal office staff Before that he was a senior research analyst with a focus on GIS and spatial statistical analysis for the Campaign Finance Institute a nonpartisan NPO in Washington DC He holds a PhD in Political Science with a focus on American politics and research methodology from the University of Maryland an MA in Political Science from Fordham University and a BA in Philosophy Psychology and Political Science from the University of Texas His research interests include American governing institutions especially Congress interest groups money and politics and quantitative methodology His current work examines how the design of political institutions shapes who wins and who loses in the policymaking process  Dr Morales teaches the Theory and Methods course on behalf of the QMSS Program He holds a PhD in Political Science from New York University and a BA in Political Science from Instituto Tecnol gico Aut nomo de M xico ITAM His current research focuses on refining the tools that have traditionally been used to measure and model Economic Voting both at the individual and at the aggregate levels He is currently a Senior Data Scientist at NBCUniversal Prior to that he served as Director General for Political Analysis for the Communications Coordinator and Federal Government Spokeswoman at the Office of the Mexican Presidency and as Spokesman for the Permanent Mission of Mexico to the United Nations during the country s most recent tenure as non permanent member of the UN Security Council His other research interests include Voting Behavior Public Opinion Comparative Politics and Quantitative Methodology  My research focuses broadly on the institutional development of American government and in applying quantitative techniques to the study of legal issues in intellectual property and administrative law My work is interdisciplinary in nature incorporating elements from law political science statistics and history My work has appeared or is forthcoming in Georgetown Law Journal Duke Law Journal Minnesota Law Review Boston College Law Review Journal of Empirical Legal Studies Berkeley Journal of Employment and Labor Law among others I am also currently working on a book project on administrative adjudication in the patent law labor law data privacy and immigration fields  Born and raised in the Bronx in New York City Aracelis Torres understood early on that all health care is not created equal This realization fueled her desire to pursue a path to address the issue of health disparities After completing her undergraduate studies at Yale University Torres work in a New York City community health center solidified her decision to apply her skills toward a career in public health She obtained her MPH from the Yale School of Public Health with a concentration in Chronic Disease Epidemiology Torres work there focused primarily on the disparities in utilization of mammography screening within the Hispanic Latino community Torres was a doctoral student at the Bloomberg School within the Cancer Epidemiology concentration There she has analyzed the effectiveness of patient navigation on improving breast cancer screening among black Medicare beneficiaries from Baltimore City She has worked toward integrating this information with geographic data to determine whether the success of the patient navigation program varies by how far individuals reside from health services She also spent time working closely with the Chief of Epidemiologic Services at the Baltimore City Health Department to create a youth violence report for the city s Violent Crime Reduction Enhancement Initiative  Dr Yang is an Adjunct Assistant Professor within the Department of Political Science and she teaches the Quantitative Theory and Methods course on behalf of the QMSS program She is a Senior Manager at Pfizer Inc where she and her team develop global strategic plans and projects using health economic and outcome research HEOR methodology and real world evidence RWE that support the value proposition marketing and market access for Pfizer oncology portfolio She works with cross functional teams to diagnose strategize and illuminate a product s benefit risk and economic profile tailored to a myriad of market stakeholders to promote healthcare system performance in an increasingly value based environment She has extensive experience in conducting complex study design performing advanced data analysis using big data such as health claims datasets to understand patients health seeking behaviors and barriers to care Dr Yang has authored over abstracts and original research articles on high impact journals in a variety of therapeutic areas Dr Yang received a Master of Health Science degree in epidemiology from the Johns Hopkins Bloomberg School of Public Health and a Doctor of Public Health degree in Epidemiology from the Mailman School of Public Health at Columbia University  Dr Houlihan is Vice President of Product Data Science for Publicis Groupe where he is responsible for building out complex machine learning frameworks and managing a team of the worlds best Data Scientists Previously he founded architect ed and deployed a financial data analytics company SentiQuant and spent fifteen years in high technology roles here he designed semiconductors into a variety of complex systems Houlihan earned his doctorate in Financial Engineering from Stevens Institute of Technology where his research focus was sentiment analysis through natural language processing and machine learning techniques He also holds both a BSEE and MBA from Drexel University His work has appeared in Quantitative Finance Computational Economics and the Journal of Investing          Quantitative Methods in the Social Sciences at Columbia University All rights reserved \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'http://www.qmss.columbia.edu/faculty-and-staff'\n",
    "\n",
    "content = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(content.text, 'html.parser')\n",
    "\n",
    "tmp_text = [re.sub('[^A-z]+', ' ' ,word.text) for word in soup.findAll('p')]\n",
    "\n",
    "tmp_text = ' '.join(tmp_text)\n",
    "\n",
    "print(tmp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Fly_fishing', 'https://www.cabelas.com/category/Fly-Fishing/104721480.uts', 'https://www.youtube.com/watch%3Fv%3DTOdvCH1vA80', 'https://www.artofmanliness.com/articles/fly-fishing/', 'https://en.m.wikipedia.org/wiki/Fly_fishing', 'https://en.m.wikipedia.org/wiki/Fly_fishing', 'https://theflyfishingbasics.com/what-is-fly-fishing/', 'http://eekwi.org/nature/flyfishin1.htm', 'https://howtoflyfish.orvis.com/', 'https://www.redington.com/experience/new-to-fly-fishing', 'https://www.oars.com/video/fly-fishing-video/', 'https://www.nps.gov/articles/fly-fishing.htm', 'https://flyfishingshow.com/edison-nj/']\n"
     ]
    }
   ],
   "source": [
    "#pip install pyyaml ua-parser user-agents fake-useragent\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def fetch_urls(query, cnt):\n",
    "    ua = UserAgent()\n",
    "    \n",
    "    google_url = 'https://www.google.com/search?q=' + query + \"&num=\" + str(cnt)\n",
    "    try:\n",
    "        response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "        \n",
    "        links = list()\n",
    "        \n",
    "        for r in result_div:\n",
    "            try:\n",
    "                link = r.find('a', href = True)\n",
    "                if link != '':\n",
    "                    print (link['href'])\n",
    "                    links.append(link['href'])  \n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        print (\"hey you\")\n",
    "        pass\n",
    "\n",
    "    #now lets use the following function that returns\n",
    "#URLs from an arbitrary regex crawl form google\n",
    "\n",
    "#pip install pyyaml ua-parser user-agents fake-useragent\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "\n",
    "def fetch_urls(query, cnt):\n",
    "    ua = UserAgent()\n",
    "\n",
    "    #query = 'fishing'\n",
    "\n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(cnt)\n",
    "    try:\n",
    "        response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "\n",
    "        links = []\n",
    "        titles = []\n",
    "        descriptions = []\n",
    "        for r in result_div:\n",
    "            # Checks if each element is present, else, raise exception\n",
    "            try:\n",
    "                link = r.find('a', href = True)\n",
    "                title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "                description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "\n",
    "                # Check to make sure everything is present before appending\n",
    "                if link != '' and title != '' and description != '': \n",
    "                    links.append(link['href'])\n",
    "                    titles.append(title)\n",
    "                    descriptions.append(description)\n",
    "            # Next loop if one element is not present\n",
    "            except:\n",
    "                continue  \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    clean_links = []\n",
    "    for i, l in enumerate(links):\n",
    "        clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
    "        clean_links.append(clean.group(1))\n",
    "        \n",
    "    return clean_links\n",
    "    \n",
    "temp = fetch_urls('fly fishing', 10)\n",
    "print (temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['cat', 'dog', 'ran', 'hill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "s_w = stopwords.words('english')\n",
    "print (s_w)\n",
    "\n",
    "sentence = 'the cat and the dog ran up the hill'\n",
    "\n",
    "rem_sw = [word for word in sentence.split() if word not in s_w]\n",
    "print (rem_sw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'down', 'laugh', 'the', 'hill', 'ran', 'run', 'unequ']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_stemmer = PorterStemmer()\n",
    "\n",
    "fun_sentence = 'running downing laughing the hill ran run unequal'\n",
    "\n",
    "tmp = word_tokenize(fun_sentence)\n",
    "tmp = ' '.join(tmp)\n",
    "\n",
    "the_stem = [my_stemmer.stem(word) for word in fun_sentence.split()]\n",
    "\n",
    "print (the_stem)\n",
    "\n",
    "#for word in fun_sentence.split():\n",
    "#    print (my_stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>fence</th>\n",
       "      <th>fox</th>\n",
       "      <th>grass</th>\n",
       "      <th>jumped</th>\n",
       "      <th>on</th>\n",
       "      <th>over</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brown  fence  fox  grass  jumped  on  over  sat  the\n",
       "0      1      1    1      0       1   0     1    0    2\n",
       "1      1      0    1      2       0   1     0    1    2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "doc1 = 'the brown fox jumped over the fence'\n",
    "doc2 = 'the brown fox sat on the grass GRASS'\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "my_vec = vectorizer.fit_transform([doc1, doc2])\n",
    "#print (vectorizer.get_feature_names())\n",
    "#print (my_vec.toarray())\n",
    "\n",
    "my_pd = pd.DataFrame(my_vec.toarray())\n",
    "my_pd.columns = vectorizer.get_feature_names()\n",
    "\n",
    "my_pd.head()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      brown    fence       fox     grass   jumped        on     over  \\\n",
      "0  0.289569  0.40698  0.289569  0.000000  0.40698  0.000000  0.40698   \n",
      "1  0.236677  0.00000  0.236677  0.665283  0.00000  0.332642  0.00000   \n",
      "\n",
      "        sat       the  \n",
      "0  0.000000  0.579139  \n",
      "1  0.332642  0.473355  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "my_vec = vectorizer.fit_transform([doc1, doc2])\n",
    "#print (vectorizer.get_feature_names())\n",
    "#print (my_vec.toarray())\n",
    "\n",
    "import pandas as pd\n",
    "my_pd = pd.DataFrame(my_vec.toarray())\n",
    "my_pd.columns = vectorizer.get_feature_names()\n",
    "print (my_pd.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
